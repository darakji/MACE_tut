{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional, Type, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode\n",
    "\n",
    "from mace.tools.scatter import scatter_sum\n",
    "\n",
    "from mace.modules.blocks import (\n",
    "    AtomicEnergiesBlock,\n",
    "    EquivariantProductBasisBlock,\n",
    "    InteractionBlock,\n",
    "    LinearNodeEmbeddingBlock,\n",
    "    LinearReadoutBlock,\n",
    "    NonLinearReadoutBlock,\n",
    "    RadialEmbeddingBlock,\n",
    "    ScaleShiftBlock,\n",
    ")\n",
    "from mace.modules.utils import (\n",
    "    get_edge_vectors_and_lengths,\n",
    "    get_outputs,\n",
    ")\n",
    "\n",
    "# pylint: disable=C0302\n",
    "\n",
    "@compile_mode(\"script\")\n",
    "class MACE(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        r_max: float,\n",
    "        num_bessel: int,\n",
    "        num_polynomial_cutoff: int,\n",
    "        max_ell: int,\n",
    "        interaction_cls: Type[InteractionBlock],\n",
    "        interaction_cls_first: Type[InteractionBlock],\n",
    "        num_interactions: int,\n",
    "        num_elements: int,\n",
    "        hidden_irreps: o3.Irreps,\n",
    "        MLP_irreps: o3.Irreps,\n",
    "        atomic_energies: np.ndarray,\n",
    "        avg_num_neighbors: float,\n",
    "        atomic_numbers: List[int],\n",
    "        correlation: Union[int, List[int]],\n",
    "        gate: Optional[Callable],\n",
    "        pair_repulsion: bool = False,\n",
    "        distance_transform: str = \"None\",\n",
    "        radial_MLP: Optional[List[int]] = None,\n",
    "        radial_type: Optional[str] = \"bessel\",\n",
    "        heads: Optional[List[str]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # buffers\n",
    "        self.register_buffer(\n",
    "            \"atomic_numbers\", torch.tensor(atomic_numbers, dtype=torch.int64)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"r_max\", torch.tensor(r_max, dtype=torch.get_default_dtype())\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"num_interactions\", torch.tensor(num_interactions, dtype=torch.int64)\n",
    "        )\n",
    "\n",
    "        # handle heads\n",
    "        if heads is None:\n",
    "            heads = [\"default\"]\n",
    "        self.heads = heads\n",
    "\n",
    "        # handle correlations\n",
    "        if isinstance(correlation, int):\n",
    "            correlation = [correlation] * num_interactions\n",
    "\n",
    "        # Embedding irreps\n",
    "        node_attr_irreps = o3.Irreps([(num_elements, (0, 1))])\n",
    "        node_feats_irreps = o3.Irreps([(hidden_irreps.count(o3.Irrep(0, 1)), (0, 1))])\n",
    "\n",
    "        # node embedding\n",
    "        self.node_embedding = LinearNodeEmbeddingBlock(\n",
    "            irreps_in=node_attr_irreps, irreps_out=node_feats_irreps\n",
    "        )\n",
    "\n",
    "        # radial embedding/bessel\n",
    "        self.radial_embedding = RadialEmbeddingBlock(\n",
    "            r_max=r_max,\n",
    "            num_bessel=num_bessel,\n",
    "            num_polynomial_cutoff=num_polynomial_cutoff,\n",
    "            radial_type=radial_type,\n",
    "            distance_transform=distance_transform,\n",
    "        )\n",
    "\n",
    "        edge_feats_irreps = o3.Irreps(f\"{self.radial_embedding.out_dim}x0e\")\n",
    "        sh_irreps = o3.Irreps.spherical_harmonics(max_ell)\n",
    "        num_features = hidden_irreps.count(o3.Irrep(0, 1))\n",
    "        interaction_irreps = (sh_irreps * num_features).sort()[0].simplify()\n",
    "\n",
    "        # spherical/angular embedding\n",
    "        self.spherical_harmonics = o3.SphericalHarmonics(\n",
    "            sh_irreps, normalize=True, normalization=\"component\"\n",
    "        )\n",
    "        if radial_MLP is None:\n",
    "            radial_MLP = [64, 64, 64]\n",
    "\n",
    "\n",
    "        # Interactions and readout\n",
    "        self.atomic_energies_fn = AtomicEnergiesBlock(atomic_energies)\n",
    "\n",
    "        inter = interaction_cls_first(\n",
    "            node_attrs_irreps=node_attr_irreps,\n",
    "            node_feats_irreps=node_feats_irreps,\n",
    "            edge_attrs_irreps=sh_irreps,\n",
    "            edge_feats_irreps=edge_feats_irreps,\n",
    "            target_irreps=interaction_irreps,\n",
    "            hidden_irreps=hidden_irreps,\n",
    "            avg_num_neighbors=avg_num_neighbors,\n",
    "            radial_MLP=radial_MLP,\n",
    "        )\n",
    "        self.interactions = torch.nn.ModuleList([inter])\n",
    "\n",
    "        # Use the appropriate self connection at the first layer for proper E0\n",
    "        use_sc_first = False\n",
    "        if \"Residual\" in str(interaction_cls_first):\n",
    "            use_sc_first = True\n",
    "\n",
    "        # First product block\n",
    "        node_feats_irreps_out = inter.target_irreps\n",
    "\n",
    "        prod = EquivariantProductBasisBlock(\n",
    "            node_feats_irreps=node_feats_irreps_out,\n",
    "            target_irreps=hidden_irreps,\n",
    "            correlation=correlation[0],\n",
    "            num_elements=num_elements,\n",
    "            use_sc=use_sc_first,\n",
    "        )\n",
    "        self.products = torch.nn.ModuleList([prod])\n",
    "\n",
    "        # first readout\n",
    "        readout = LinearReadoutBlock(hidden_irreps, o3.Irreps(f\"{len(heads)}x0e\"))\n",
    "        self.readouts = torch.nn.ModuleList([readout])\n",
    "\n",
    "\n",
    "        for i in range(num_interactions - 1):\n",
    "            # for last layer, select only scalers\n",
    "            if i == num_interactions - 2:\n",
    "                hidden_irreps_out = str(\n",
    "                    hidden_irreps[0]\n",
    "                )  # Select only scalars for last layer\n",
    "            else:\n",
    "                hidden_irreps_out = hidden_irreps\n",
    "            \n",
    "            # get interaction block\n",
    "            inter = interaction_cls(\n",
    "                node_attrs_irreps=node_attr_irreps,\n",
    "                node_feats_irreps=hidden_irreps,\n",
    "                edge_attrs_irreps=sh_irreps,\n",
    "                edge_feats_irreps=edge_feats_irreps,\n",
    "                target_irreps=interaction_irreps,\n",
    "                hidden_irreps=hidden_irreps_out,\n",
    "                avg_num_neighbors=avg_num_neighbors,\n",
    "                radial_MLP=radial_MLP,\n",
    "            )\n",
    "            self.interactions.append(inter)\n",
    "\n",
    "            # get product block\n",
    "            prod = EquivariantProductBasisBlock(\n",
    "                node_feats_irreps=interaction_irreps,\n",
    "                target_irreps=hidden_irreps_out,\n",
    "                correlation=correlation[i + 1],\n",
    "                num_elements=num_elements,\n",
    "                use_sc=True,\n",
    "            )\n",
    "            self.products.append(prod)\n",
    "\n",
    "            # get readout\n",
    "            # for last layer, non linear readout\n",
    "            if i == num_interactions - 2:\n",
    "                self.readouts.append(\n",
    "                    NonLinearReadoutBlock(\n",
    "                        hidden_irreps_out,\n",
    "                        (len(heads) * MLP_irreps).simplify(),\n",
    "                        gate,\n",
    "                        o3.Irreps(f\"{len(heads)}x0e\"),\n",
    "                        len(heads),\n",
    "                    )\n",
    "                )\n",
    "            # for other layers linear layout\n",
    "            else:\n",
    "                self.readouts.append(\n",
    "                    LinearReadoutBlock(hidden_irreps, o3.Irreps(f\"{len(heads)}x0e\"))\n",
    "                )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data: Dict[str, torch.Tensor],\n",
    "        training: bool = False,\n",
    "        compute_force: bool = True,\n",
    "        compute_virials: bool = False,\n",
    "        compute_stress: bool = False,\n",
    "        compute_displacement: bool = False,\n",
    "        compute_hessian: bool = False,\n",
    "    ) -> Dict[str, Optional[torch.Tensor]]:\n",
    "\n",
    "        # Setup\n",
    "        data[\"node_attrs\"].requires_grad_(True) # (num_nodes, num_elements)\n",
    "        data[\"positions\"].requires_grad_(True)  # (num_nodes, 3)\n",
    "\n",
    "        num_atoms_arange = torch.arange(data[\"positions\"].shape[0])     # (num_nodes,)\n",
    "        # ptr = (batch_size+1), [0,12,..], each ptr[i] shows starting index of ith molecule/graph\n",
    "        num_graphs = data[\"ptr\"].numel() - 1        # int: batch_size=num_graphs\n",
    "\n",
    "        node_heads = (\n",
    "            data[\"head\"][data[\"batch\"]]\n",
    "            if \"head\" in data\n",
    "            else torch.zeros_like(data[\"batch\"])\n",
    "        )   # (num_nodes) int32, each idx shows which head that node belongs to\n",
    "\n",
    "        # (batch_size, 3, 3)\n",
    "        displacement = torch.zeros(\n",
    "            (num_graphs, 3, 3),\n",
    "            dtype=data[\"positions\"].dtype,\n",
    "            device=data[\"positions\"].device,\n",
    "        )\n",
    "\n",
    "        # Atomic energies\n",
    "        # self.atomic_energies_fn(data[\"node_attrs\"]) => (num_nodes, num_heads)\n",
    "        node_e0 = self.atomic_energies_fn(data[\"node_attrs\"])[\n",
    "            num_atoms_arange, node_heads\n",
    "        ]   # nodes_e0 = (num_nodes), selects only the energy of head under which node comes\n",
    "\n",
    "        # e0 (batch_size) => sums up energy of nodes in each molecule\n",
    "        e0 = scatter_sum(\n",
    "            src=node_e0, index=data[\"batch\"], dim=0, dim_size=num_graphs\n",
    "        )  \n",
    "        \n",
    "\n",
    "        # Embeddings \n",
    "        # node embedding/features (num_nodes, num_channels_of_0e in hidden rep)\n",
    "        node_feats = self.node_embedding(data[\"node_attrs\"])    \n",
    "\n",
    "        # vectors (num_edges,3), lengths (num_edges,1)\n",
    "        vectors, lengths = get_edge_vectors_and_lengths(\n",
    "            positions=data[\"positions\"],\n",
    "            edge_index=data[\"edge_index\"],\n",
    "            shifts=data[\"shifts\"],\n",
    "        )\n",
    "        # edge_attrs=angular_emb = (num_edges, (max_ell+1)^2)\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        # edge_feats= radial_emb/bessel (num_edges, num_bessels), with polynomial cutoff applied\n",
    "        edge_feats = self.radial_embedding(\n",
    "            lengths, data[\"node_attrs\"], data[\"edge_index\"], self.atomic_numbers\n",
    "        )\n",
    "\n",
    "\n",
    "        # Interactions\n",
    "        energies = [e0]\n",
    "        node_energies_list = [node_e0]\n",
    "        node_feats_list = []\n",
    "\n",
    "        for interaction, product, readout in zip(\n",
    "            self.interactions, self.products, self.readouts\n",
    "        ):\n",
    "            # node_feats (num_nodes, num_channels, (max_ell+1)^2)\n",
    "            # sc (num_nodes, hidden_irrep.dim())\n",
    "            # RadialMLP is applied inside\n",
    "            node_feats, sc = interaction(\n",
    "                node_attrs=data[\"node_attrs\"],\n",
    "                node_feats=node_feats,\n",
    "                edge_attrs=edge_attrs,\n",
    "                edge_feats=edge_feats,\n",
    "                edge_index=data[\"edge_index\"],\n",
    "            )\n",
    "            \n",
    "            # node_feats (num_nodes, hidden_irrep.dim())\n",
    "            node_feats = product(\n",
    "                node_feats=node_feats,\n",
    "                sc=sc,\n",
    "                node_attrs=data[\"node_attrs\"],\n",
    "            )\n",
    "            node_feats_list.append(node_feats)\n",
    "\n",
    "            # readout (num_nodes, num_heads)\n",
    "            # node_energies (num_nodes), only the energy corresponding to head of config is selected\n",
    "            node_energies = readout(node_feats, node_heads)[\n",
    "                num_atoms_arange, node_heads\n",
    "            ] \n",
    "            # energy (batch_size)\n",
    "            energy = scatter_sum(\n",
    "                src=node_energies,\n",
    "                index=data[\"batch\"],\n",
    "                dim=0,\n",
    "                dim_size=num_graphs,\n",
    "            )\n",
    "\n",
    "            energies.append(energy)\n",
    "            node_energies_list.append(node_energies)\n",
    "\n",
    "        # Concatenate node features\n",
    "        # (num_nodes, (num_interactions-1)*hidden_irreps.dim() + scalar_channels_hidden_reps)\n",
    "        node_feats_out = torch.cat(node_feats_list, dim=-1)\n",
    "\n",
    "        # Sum over energy contributions\n",
    "        # in contribution, 1 from atomic_energies and other from each readout of interactions\n",
    "        contributions = torch.stack(energies, dim=-1)   #(batch_size, 1+num_interactions)\n",
    "        total_energy = torch.sum(contributions, dim=-1)  # (batch_size)\n",
    "        node_energy_contributions = torch.stack(node_energies_list, dim=-1) #(batch_size, num_nodes)\n",
    "        node_energy = torch.sum(node_energy_contributions, dim=-1)  # (num_nodes)\n",
    "\n",
    "\n",
    "        # Outputs\n",
    "        forces, virials, stress, hessian = get_outputs(\n",
    "            energy=total_energy,\n",
    "            positions=data[\"positions\"],\n",
    "            displacement=displacement,\n",
    "            cell=data[\"cell\"],\n",
    "            training=training,\n",
    "            compute_force=compute_force,\n",
    "            compute_virials=compute_virials,\n",
    "            compute_stress=compute_stress,\n",
    "            compute_hessian=compute_hessian,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"energy\": total_energy, # (batch_size)\n",
    "            \"node_energy\": node_energy, #(num_nodes)\n",
    "            \"contributions\": contributions, #(batch_size, 1+num_interactions)\n",
    "            \"forces\": forces,   #(num_nodes, 3)\n",
    "            \"virials\": virials,\n",
    "            \"stress\": stress,\n",
    "            \"displacement\": displacement,\n",
    "            \"hessian\": hessian,\n",
    "            \"node_feats\": node_feats_out,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@compile_mode(\"script\")\n",
    "class ScaleShiftMACE(MACE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        atomic_inter_scale: float,\n",
    "        atomic_inter_shift: float,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        # scale shift module\n",
    "        self.scale_shift = ScaleShiftBlock(\n",
    "            scale=atomic_inter_scale, shift=atomic_inter_shift\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data: Dict[str, torch.Tensor],\n",
    "        training: bool = False,\n",
    "        compute_force: bool = True,\n",
    "        compute_virials: bool = False,\n",
    "        compute_stress: bool = False,\n",
    "        compute_displacement: bool = False,\n",
    "        compute_hessian: bool = False,\n",
    "    ) -> Dict[str, Optional[torch.Tensor]]:\n",
    "        \n",
    "        # Setup\n",
    "        data[\"positions\"].requires_grad_(True) # (num_nodes, num_elements)\n",
    "        data[\"node_attrs\"].requires_grad_(True) # (num_nodes, 3)\n",
    "\n",
    "        num_atoms_arange = torch.arange(data[\"positions\"].shape[0])     # (num_nodes,)\n",
    "        # ptr = (batch_size+1), [0,12,..], each ptr[i] shows starting index of ith molecule/graph\n",
    "        num_graphs = data[\"ptr\"].numel() - 1        # int: batch_size=num_graphs\n",
    "\n",
    "        node_heads = (\n",
    "            data[\"head\"][data[\"batch\"]]\n",
    "            if \"head\" in data\n",
    "            else torch.zeros_like(data[\"batch\"])\n",
    "        )   # (num_nodes) int32, each idx shows which head that node belongs to\n",
    "\n",
    "        # (batch_size, 3, 3)\n",
    "        displacement = torch.zeros(\n",
    "            (num_graphs, 3, 3),\n",
    "            dtype=data[\"positions\"].dtype,\n",
    "            device=data[\"positions\"].device,\n",
    "        )\n",
    "\n",
    "\n",
    "        # Atomic energies\n",
    "        # self.atomic_energies_fn(data[\"node_attrs\"]) => (num_nodes, num_heads)\n",
    "        node_e0 = self.atomic_energies_fn(data[\"node_attrs\"])[\n",
    "            num_atoms_arange, node_heads\n",
    "        ] # nodes_e0 = (num_nodes), selects only the energy of head under which node comes\n",
    "\n",
    "        # e0 (batch_size) => sums up energy of nodes in each molecule\n",
    "        e0 = scatter_sum(\n",
    "            src=node_e0, index=data[\"batch\"], dim=0, dim_size=num_graphs\n",
    "        )  \n",
    "\n",
    "        # Embeddings \n",
    "        # node embedding/features (num_nodes, num_channels_of_0e in hidden rep)\n",
    "        node_feats = self.node_embedding(data[\"node_attrs\"])    \n",
    "\n",
    "        # vectors (num_edges,3), lengths (num_edges,1)\n",
    "        vectors, lengths = get_edge_vectors_and_lengths(\n",
    "            positions=data[\"positions\"],\n",
    "            edge_index=data[\"edge_index\"],\n",
    "            shifts=data[\"shifts\"],\n",
    "        )\n",
    "        # edge_attrs=angular_emb = (num_edges, (max_ell+1)^2)\n",
    "        edge_attrs = self.spherical_harmonics(vectors)\n",
    "        # edge_feats= radial_emb/bessel (num_edges, num_bessels), with polynomial cutoff applied\n",
    "        edge_feats = self.radial_embedding(\n",
    "            lengths, data[\"node_attrs\"], data[\"edge_index\"], self.atomic_numbers\n",
    "        )\n",
    "        \n",
    "        # Interactions\n",
    "        node_es_list = []\n",
    "        node_feats_list = []\n",
    "        for interaction, product, readout in zip(\n",
    "            self.interactions, self.products, self.readouts\n",
    "        ):\n",
    "            # node_feats (num_nodes, num_channels, (max_ell+1)^2)\n",
    "            # sc (num_nodes, hidden_irrep.dim())\n",
    "            # RadialMLP is applied inside\n",
    "            node_feats, sc = interaction(\n",
    "                node_attrs=data[\"node_attrs\"],\n",
    "                node_feats=node_feats,\n",
    "                edge_attrs=edge_attrs,\n",
    "                edge_feats=edge_feats,\n",
    "                edge_index=data[\"edge_index\"],\n",
    "            )\n",
    "\n",
    "            # node_feats (num_nodes, hidden_irrep.dim())\n",
    "            node_feats = product(\n",
    "                node_feats=node_feats, sc=sc, node_attrs=data[\"node_attrs\"]\n",
    "            )\n",
    "            node_feats_list.append(node_feats)\n",
    "\n",
    "            # readout (num_nodes, num_heads)\n",
    "            # node_energies (num_nodes), only the energy corresponding to head of config is selected\n",
    "            node_es_list.append(\n",
    "                readout(node_feats, node_heads)[num_atoms_arange, node_heads]\n",
    "            )  # [(n_nodes, ), ] -> node_es_list = (num_interactions, n_nodes)\n",
    "\n",
    "\n",
    "        # Concatenate node features\n",
    "        # (num_nodes, (num_interactions-1)*hidden_irreps.dim() + scalar_channels_hidden_reps)\n",
    "        node_feats_out = torch.cat(node_feats_list, dim=-1)\n",
    "\n",
    "        # Sum over interactions\n",
    "        # torch.stack(node_es_list, dim=0) (num_interactions, n_nodes)\n",
    "        # interaction energies\n",
    "        node_inter_es = torch.sum(torch.stack(node_es_list, dim=0), dim=0)  # (n_nodes, )\n",
    "        # scale it\n",
    "        node_inter_es = self.scale_shift(node_inter_es, node_heads)  # (n_nodes,)\n",
    "\n",
    "        # Sum over nodes in graph\n",
    "        # (batch_size,)\n",
    "        inter_e = scatter_sum(\n",
    "            src=node_inter_es, index=data[\"batch\"], dim=-1, dim_size=num_graphs\n",
    "        ) \n",
    "\n",
    "        # Add E_0 and (scaled) interaction energy\n",
    "        total_energy = e0 + inter_e #(batch_size)\n",
    "        node_energy = node_e0 + node_inter_es   #(num_nodes)\n",
    "\n",
    "        # outputs\n",
    "        forces, virials, stress, hessian = get_outputs(\n",
    "            energy=inter_e,\n",
    "            positions=data[\"positions\"],\n",
    "            displacement=displacement,\n",
    "            cell=data[\"cell\"],\n",
    "            training=training,\n",
    "            compute_force=compute_force,\n",
    "            compute_virials=compute_virials,\n",
    "            compute_stress=compute_stress,\n",
    "            compute_hessian=compute_hessian,\n",
    "        )\n",
    "        output = {\n",
    "            \"energy\": total_energy, # (batch_size)\n",
    "            \"node_energy\": node_energy, #(num_nodes)\n",
    "            \"interaction_energy\": inter_e, #(batch_size, 1+num_interactions)\n",
    "            \"forces\": forces,  #(num_nodes, 3)\n",
    "            \"virials\": virials,\n",
    "            \"stress\": stress,\n",
    "            \"hessian\": hessian,\n",
    "            \"displacement\": displacement,\n",
    "            \"node_feats\": node_feats_out,\n",
    "        }\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# Training script\n",
    "# Authors: Ilyes Batatia, Gregor Simm, David Kovacs\n",
    "# This program is distributed under the MIT License (see MIT.md)\n",
    "###########################################################################################\n",
    "\n",
    "import dataclasses\n",
    "import logging\n",
    "import time\n",
    "from contextlib import nullcontext\n",
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torch.optim.swa_utils import SWALR, AveragedModel\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "from torchmetrics import Metric\n",
    "\n",
    "from mace.tools import torch_geometric\n",
    "from mace.tools.checkpoint import CheckpointHandler, CheckpointState\n",
    "from mace.tools.torch_tools import to_numpy\n",
    "from mace.tools.utils import (\n",
    "    MetricsLogger,\n",
    "    compute_mae,\n",
    "    compute_q95,\n",
    "    compute_rel_mae,\n",
    "    compute_rel_rmse,\n",
    "    compute_rmse,\n",
    ")\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class SWAContainer:\n",
    "    model: AveragedModel\n",
    "    scheduler: SWALR\n",
    "    start: int\n",
    "    loss_fn: torch.nn.Module\n",
    "\n",
    "\n",
    "def valid_err_log(\n",
    "    valid_loss,\n",
    "    eval_metrics,\n",
    "    logger,\n",
    "    log_errors,\n",
    "    epoch=None,\n",
    "    valid_loader_name=\"Default\",\n",
    "):\n",
    "    eval_metrics[\"mode\"] = \"eval\"\n",
    "    eval_metrics[\"epoch\"] = epoch\n",
    "    logger.log(eval_metrics)\n",
    "\n",
    "    if epoch is None:\n",
    "        inintial_phrase = \"Initial\"\n",
    "    else:\n",
    "        inintial_phrase = f\"Epoch {epoch}\"\n",
    "        \n",
    "    if log_errors == \"PerAtomRMSE\":\n",
    "        error_e = eval_metrics[\"rmse_e_per_atom\"] * 1e3\n",
    "        error_f = eval_metrics[\"rmse_f\"] * 1e3\n",
    "        logging.info(\n",
    "            f\"{inintial_phrase}: head: {valid_loader_name}, loss={valid_loss:8.8f}, RMSE_E_per_atom={error_e:8.2f} meV, RMSE_F={error_f:8.2f} meV / A\"\n",
    "        )\n",
    "    \n",
    "    elif log_errors == \"TotalRMSE\":\n",
    "        error_e = eval_metrics[\"rmse_e\"] * 1e3\n",
    "        error_f = eval_metrics[\"rmse_f\"] * 1e3\n",
    "        logging.info(\n",
    "            f\"{inintial_phrase}: head: {valid_loader_name}, loss={valid_loss:8.8f}, RMSE_E={error_e:8.2f} meV, RMSE_F={error_f:8.2f} meV / A\",\n",
    "        )\n",
    "    elif log_errors == \"PerAtomMAE\":\n",
    "        error_e = eval_metrics[\"mae_e_per_atom\"] * 1e3\n",
    "        error_f = eval_metrics[\"mae_f\"] * 1e3\n",
    "        logging.info(\n",
    "            f\"{inintial_phrase}: head: {valid_loader_name}, loss={valid_loss:8.8f}, MAE_E_per_atom={error_e:8.2f} meV, MAE_F={error_f:8.2f} meV / A\",\n",
    "        )\n",
    "    elif log_errors == \"TotalMAE\":\n",
    "        error_e = eval_metrics[\"mae_e\"] * 1e3\n",
    "        error_f = eval_metrics[\"mae_f\"] * 1e3\n",
    "        logging.info(\n",
    "            f\"{inintial_phrase}: head: {valid_loader_name}, loss={valid_loss:8.8f}, MAE_E={error_e:8.2f} meV, MAE_F={error_f:8.2f} meV / A\",\n",
    "        )\n",
    "    \n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    valid_loaders: Dict[str, DataLoader],\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    lr_scheduler: torch.optim.lr_scheduler.ExponentialLR,\n",
    "    start_epoch: int,\n",
    "    max_num_epochs: int,\n",
    "    patience: int,\n",
    "    checkpoint_handler: CheckpointHandler,\n",
    "    logger: MetricsLogger,\n",
    "    eval_interval: int,\n",
    "    output_args: Dict[str, bool],\n",
    "    device: torch.device,\n",
    "    log_errors: str,\n",
    "    swa: Optional[SWAContainer] = None,\n",
    "    ema: Optional[ExponentialMovingAverage] = None,\n",
    "    max_grad_norm: Optional[float] = 10.0,\n",
    "):\n",
    "    # some values to tracks\n",
    "    lowest_loss = np.inf\n",
    "    valid_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    swa_start = True\n",
    "    keep_last = False\n",
    "\n",
    "    if max_grad_norm is not None:\n",
    "        logging.info(f\"Using gradient clipping with tolerance={max_grad_norm:.3f}\")\n",
    "\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"===========TRAINING===========\")\n",
    "    logging.info(\"Started training, reporting errors on validation set\")\n",
    "    logging.info(\"Loss metrics on validation set\")\n",
    "    epoch = start_epoch\n",
    "\n",
    "    # log validation loss before _any_ training\n",
    "    valid_loss = 0.0\n",
    "    for valid_loader_name, valid_loader in valid_loaders.items():\n",
    "        # evaluate val loss and metrics for each val_dataloader and log them\n",
    "        valid_loss_head, eval_metrics = evaluate(\n",
    "                                                    model=model,\n",
    "                                                    loss_fn=loss_fn,\n",
    "                                                    data_loader=valid_loader,\n",
    "                                                    output_args=output_args,\n",
    "                                                    device=device,\n",
    "                                                )\n",
    "        valid_err_log(\n",
    "                        valid_loss_head, eval_metrics, logger, log_errors, None, valid_loader_name\n",
    "                    )\n",
    "    valid_loss = valid_loss_head  # consider only the last head for the checkpoint\n",
    "\n",
    "    # till epoch is less than max_num_epochs\n",
    "    while epoch < max_num_epochs:\n",
    "        # LR scheduler and SWA update\n",
    "        if swa is None or epoch < swa.start:\n",
    "            if epoch > start_epoch:\n",
    "                lr_scheduler.step(\n",
    "                    metrics=valid_loss\n",
    "                )  # Can break if exponential LR, TODO fix that!\n",
    "        else:\n",
    "            # swa_starts, so change loss and load model\n",
    "            if swa_start:\n",
    "                logging.info(\"Changing loss based on Stage Two Weights\")\n",
    "                lowest_loss = np.inf\n",
    "                swa_start = False\n",
    "                keep_last = True\n",
    "            loss_fn = swa.loss_fn\n",
    "            swa.model.update_parameters(model)\n",
    "            if epoch > start_epoch:\n",
    "                swa.scheduler.step()\n",
    "\n",
    "        # Train\n",
    "        if \"ScheduleFree\" in type(optimizer).__name__:\n",
    "            optimizer.train()\n",
    "        \n",
    "        # trains one epoch\n",
    "        train_one_epoch(\n",
    "            model=model,\n",
    "            loss_fn=loss_fn,\n",
    "            data_loader=train_loader,\n",
    "            optimizer=optimizer,\n",
    "            epoch=epoch,\n",
    "            output_args=output_args,\n",
    "            max_grad_norm=max_grad_norm,\n",
    "            ema=ema,\n",
    "            logger=logger,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        # Validate\n",
    "        if epoch % eval_interval == 0:\n",
    "            model_to_evaluate = model\n",
    "            param_context = (\n",
    "                ema.average_parameters() if ema is not None else nullcontext()\n",
    "            )\n",
    "            if \"ScheduleFree\" in type(optimizer).__name__:\n",
    "                optimizer.eval()\n",
    "            # evaluate model on val loader, for each val loader\n",
    "            with param_context:\n",
    "                valid_loss = 0.0\n",
    "                for valid_loader_name, valid_loader in valid_loaders.items():\n",
    "                    valid_loss_head, eval_metrics = evaluate(\n",
    "                                                                model=model_to_evaluate,\n",
    "                                                                loss_fn=loss_fn,\n",
    "                                                                data_loader=valid_loader,\n",
    "                                                                output_args=output_args,\n",
    "                                                                device=device,\n",
    "                                                            )\n",
    "                    # log the val metrics\n",
    "                    valid_err_log(\n",
    "                            valid_loss_head,\n",
    "                            eval_metrics,\n",
    "                            logger,\n",
    "                            log_errors,\n",
    "                            epoch,\n",
    "                            valid_loader_name,\n",
    "                        )\n",
    "                valid_loss = (\n",
    "                    valid_loss_head  # consider only the last head for the checkpoint\n",
    "                )\n",
    "\n",
    "            # if val loss increase\n",
    "            if valid_loss >= lowest_loss:\n",
    "                # increase patience counter\n",
    "                patience_counter += 1\n",
    "                # terminate if patience counter exceeds patience\n",
    "                if patience_counter >= patience:\n",
    "                    if swa is not None and epoch < swa.start:\n",
    "                        logging.info(\n",
    "                            f\"Stopping optimization after {patience_counter} epochs without improvement and starting Stage Two\"\n",
    "                        )\n",
    "                        epoch = swa.start\n",
    "                    else:\n",
    "                        logging.info(\n",
    "                            f\"Stopping optimization after {patience_counter} epochs without improvement\"\n",
    "                        )\n",
    "                        break\n",
    "\n",
    "\n",
    "            # val loss decreased, reset patience counter and save model\n",
    "            else:\n",
    "                lowest_loss = valid_loss\n",
    "                patience_counter = 0\n",
    "                param_context = (\n",
    "                    ema.average_parameters() if ema is not None else nullcontext()\n",
    "                )\n",
    "                with param_context:\n",
    "                    checkpoint_handler.save(\n",
    "                        state=CheckpointState(model, optimizer, lr_scheduler),\n",
    "                        epochs=epoch,\n",
    "                        keep_last=keep_last,\n",
    "                    )\n",
    "                    keep_last = False\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    logging.info(\"Training complete\")\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "                    model: torch.nn.Module,\n",
    "                    loss_fn: torch.nn.Module,\n",
    "                    data_loader: DataLoader,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    epoch: int,\n",
    "                    output_args: Dict[str, bool],\n",
    "                    max_grad_norm: Optional[float],\n",
    "                    ema: Optional[ExponentialMovingAverage],\n",
    "                    logger: MetricsLogger,\n",
    "                    device: torch.device,\n",
    "                ) -> None:\n",
    "    \n",
    "    # iterate thorugh batches and take step\n",
    "    for batch in data_loader:\n",
    "        _, opt_metrics = take_step(\n",
    "                                    model=model,\n",
    "                                    loss_fn=loss_fn,\n",
    "                                    batch=batch,\n",
    "                                    optimizer=optimizer,\n",
    "                                    ema=ema,\n",
    "                                    output_args=output_args,\n",
    "                                    max_grad_norm=max_grad_norm,\n",
    "                                    device=device,\n",
    "                                )\n",
    "        opt_metrics[\"mode\"] = \"opt\"\n",
    "        opt_metrics[\"epoch\"] = epoch\n",
    "        logger.log(opt_metrics)\n",
    "\n",
    "\n",
    "def take_step(  model: torch.nn.Module,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                batch: torch_geometric.batch.Batch,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                ema: Optional[ExponentialMovingAverage],\n",
    "                output_args: Dict[str, bool],\n",
    "                max_grad_norm: Optional[float],\n",
    "                device: torch.device,\n",
    "            ) -> Tuple[float, Dict[str, Any]]:\n",
    "    # measure time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # send batch to device\n",
    "    batch = batch.to(device)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    batch_dict = batch.to_dict()\n",
    "\n",
    "    output = model(\n",
    "                    batch_dict,\n",
    "                    training=True,\n",
    "                    compute_force=output_args[\"forces\"],\n",
    "                    compute_virials=output_args[\"virials\"],\n",
    "                    compute_stress=output_args[\"stress\"],\n",
    "                )\n",
    "    loss = loss_fn(pred=output, ref=batch)\n",
    "    loss.backward()\n",
    "\n",
    "    if max_grad_norm is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    if ema is not None:\n",
    "        ema.update()\n",
    "\n",
    "    loss_dict = {\n",
    "        \"loss\": to_numpy(loss),\n",
    "        \"time\": time.time() - start_time,\n",
    "    }\n",
    "\n",
    "    return loss, loss_dict\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "            model: torch.nn.Module,\n",
    "            loss_fn: torch.nn.Module,\n",
    "            data_loader: DataLoader,\n",
    "            output_args: Dict[str, bool],\n",
    "            device: torch.device,\n",
    "        ) -> Tuple[float, Dict[str, Any]]:\n",
    "    \n",
    "    # freeze the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # create metrics\n",
    "    metrics = MACELoss(loss_fn=loss_fn).to(device)\n",
    "\n",
    "    # start the timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        batch = batch.to(device)\n",
    "        batch_dict = batch.to_dict()\n",
    "        output = model(\n",
    "                    batch_dict,\n",
    "                    training=False,\n",
    "                    compute_force=output_args[\"forces\"],\n",
    "                    compute_virials=output_args[\"virials\"],\n",
    "                    compute_stress=output_args[\"stress\"],\n",
    "                )\n",
    "        avg_loss, aux = metrics(batch, output)\n",
    "\n",
    "    avg_loss, aux = metrics.compute()\n",
    "    aux[\"time\"] = time.time() - start_time\n",
    "    metrics.reset()\n",
    "\n",
    "    # unfreeze the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return avg_loss, aux\n",
    "\n",
    "\n",
    "class MACELoss(Metric):\n",
    "    def __init__(self, loss_fn: torch.nn.Module):\n",
    "        super().__init__()\n",
    "        self.loss_fn = loss_fn\n",
    "        self.add_state(\"total_loss\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"num_data\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"E_computed\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"delta_es\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"delta_es_per_atom\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"Fs_computed\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")\n",
    "        self.add_state(\"fs\", default=[], dist_reduce_fx=\"cat\")\n",
    "        self.add_state(\"delta_fs\", default=[], dist_reduce_fx=\"cat\")\n",
    "\n",
    "    def update(self, batch, output):  # pylint: disable=arguments-differ\n",
    "        loss = self.loss_fn(pred=output, ref=batch)\n",
    "        self.total_loss += loss\n",
    "        self.num_data += batch.num_graphs\n",
    "\n",
    "        if output.get(\"energy\") is not None and batch.energy is not None:\n",
    "            self.E_computed += 1.0\n",
    "            self.delta_es.append(batch.energy - output[\"energy\"])\n",
    "            self.delta_es_per_atom.append(\n",
    "                (batch.energy - output[\"energy\"]) / (batch.ptr[1:] - batch.ptr[:-1])\n",
    "            )\n",
    "        if output.get(\"forces\") is not None and batch.forces is not None:\n",
    "            self.Fs_computed += 1.0\n",
    "            self.fs.append(batch.forces)\n",
    "            self.delta_fs.append(batch.forces - output[\"forces\"])\n",
    "        \n",
    "\n",
    "    def convert(self, delta: Union[torch.Tensor, List[torch.Tensor]]) -> np.ndarray:\n",
    "        if isinstance(delta, list):\n",
    "            delta = torch.cat(delta)\n",
    "        return to_numpy(delta)\n",
    "\n",
    "    def compute(self):\n",
    "        aux = {}\n",
    "        aux[\"loss\"] = to_numpy(self.total_loss / self.num_data).item()\n",
    "        if self.E_computed:\n",
    "            delta_es = self.convert(self.delta_es)\n",
    "            delta_es_per_atom = self.convert(self.delta_es_per_atom)\n",
    "            aux[\"mae_e\"] = compute_mae(delta_es)\n",
    "            aux[\"mae_e_per_atom\"] = compute_mae(delta_es_per_atom)\n",
    "            aux[\"rmse_e\"] = compute_rmse(delta_es)\n",
    "            aux[\"rmse_e_per_atom\"] = compute_rmse(delta_es_per_atom)\n",
    "            aux[\"q95_e\"] = compute_q95(delta_es)\n",
    "        if self.Fs_computed:\n",
    "            fs = self.convert(self.fs)\n",
    "            delta_fs = self.convert(self.delta_fs)\n",
    "            aux[\"mae_f\"] = compute_mae(delta_fs)\n",
    "            aux[\"rel_mae_f\"] = compute_rel_mae(delta_fs, fs)\n",
    "            aux[\"rmse_f\"] = compute_rmse(delta_fs)\n",
    "            aux[\"rel_rmse_f\"] = compute_rel_rmse(delta_fs, fs)\n",
    "            aux[\"q95_f\"] = compute_q95(delta_fs)\n",
    "        \n",
    "        return aux[\"loss\"], aux\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################################\n",
    "# Training script for MACE\n",
    "# Authors: Ilyes Batatia, Gregor Simm, David Kovacs\n",
    "# This program is distributed under the MIT License (see MIT.md)\n",
    "###########################################################################################\n",
    "\n",
    "import argparse\n",
    "import ast\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import torch.distributed\n",
    "import torch.nn.functional\n",
    "from torch.utils.data import ConcatDataset\n",
    "from torch_ema import ExponentialMovingAverage\n",
    "\n",
    "import mace\n",
    "from mace import data, tools\n",
    "from mace.tools import torch_geometric\n",
    "from mace.tools.model_script_utils import configure_model\n",
    "from mace.tools.multihead_tools import (\n",
    "    HeadConfig,\n",
    "    dict_head_to_dataclass,\n",
    "    prepare_default_head,\n",
    ")\n",
    "from mace.tools.scripts_utils import (\n",
    "    LRScheduler,\n",
    "    check_path_ase_read,\n",
    "    dict_to_array,\n",
    "    get_atomic_energies,\n",
    "    get_avg_num_neighbors,\n",
    "    get_config_type_weights,\n",
    "    get_dataset_from_xyz,\n",
    "    get_loss_fn,\n",
    "    get_optimizer,\n",
    "    get_params_options,\n",
    "    get_swa,\n",
    "    remove_pt_head,\n",
    ")\n",
    "from mace.tools.tables_utils import create_error_table\n",
    "from mace.tools.utils import AtomicNumberTable\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    This script runs the training/fine tuning for mace\n",
    "    \"\"\"\n",
    "    args = tools.build_default_arg_parser().parse_args()\n",
    "    run(args)\n",
    "\n",
    "\n",
    "def run(args: argparse.Namespace) -> None:\n",
    "    \"\"\"\n",
    "    This script runs the training/fine tuning for mace\n",
    "    \"\"\"\n",
    "    # tag is like model name\n",
    "    tag = tools.get_tag(name=args.name, seed=args.seed)\n",
    "\n",
    "    # check all args\n",
    "    args, input_log_messages = tools.check_args(args)\n",
    "\n",
    "    # Setup\n",
    "    tools.set_seeds(args.seed)\n",
    "    tools.setup_logger(level=args.log_level, tag=tag, directory=args.log_dir)\n",
    "    logging.info(\"===========VERIFYING SETTINGS===========\")\n",
    "    for message, loglevel in input_log_messages:\n",
    "        logging.log(level=loglevel, msg=message)\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"MACE version: {mace.__version__}\")\n",
    "    except AttributeError:\n",
    "        logging.info(\"Cannot find MACE version, please install MACE via pip\")\n",
    "    logging.debug(f\"Configuration: {args}\")\n",
    "\n",
    "    # setup dtype, init device\n",
    "    tools.set_default_dtype(args.default_dtype)\n",
    "    device = tools.init_device(args.device)\n",
    "    \n",
    "    # see if pretrained model is given and can we use multihead finetuning\n",
    "    model_foundation: Optional[torch.nn.Module] = None\n",
    "    if args.foundation_model is not None:\n",
    "        assert os.path.exists(args.foundation_model), f\"Couldn't find the model at path {args.foundation_model}\"\n",
    "\n",
    "        # load the model\n",
    "        model_foundation = torch.load(args.foundation_model, map_location=args.device)\n",
    "        logging.info(f\"Using foundation model {args.foundation_model} as initial checkpoint.\")\n",
    "        args.r_max = model_foundation.r_max.item()\n",
    "        # if pretraining file in not provided, can't do multihead finetuning\n",
    "        if args.pt_train_file is None:\n",
    "            logging.warning(\"Using multiheads finetuning with a foundation model that is not a Materials Project model, need to provied a path to a pretraining file with --pt_train_file.\")\n",
    "            args.multiheads_finetuning = False\n",
    "        \n",
    "        # if multihead finetuning is selected\n",
    "        if args.multiheads_finetuning:\n",
    "            assert args.E0s != \"average\", \"average atomic energies cannot be used for multiheads finetuning\"\n",
    "            # check that the foundation model has a single head, if not, use the first head\n",
    "            if hasattr(model_foundation, \"heads\"):\n",
    "                if len(model_foundation.heads) > 1:\n",
    "                    logging.warning(\"Mutlihead finetuning with models with more than one head is not supported, using the first head as foundation head.\")\n",
    "                    model_foundation = remove_pt_head(model_foundation, args.foundation_head)\n",
    "    else:\n",
    "        args.multiheads_finetuning = False\n",
    "\n",
    "    # if head is provided, use, else prepare\n",
    "    # head is dict(str:things), eg train_file, valid_file, E0 etc\n",
    "    if args.heads is not None:\n",
    "        args.heads = ast.literal_eval(args.heads)\n",
    "    else:\n",
    "        args.heads = prepare_default_head(args)\n",
    "\n",
    "    # load input data for each head one by one\n",
    "    logging.info(\"===========LOADING INPUT DATA===========\")\n",
    "    heads = list(args.heads.keys())\n",
    "    logging.info(f\"Using heads: {heads}\")\n",
    "    head_configs: List[HeadConfig] = []\n",
    "\n",
    "    for head, head_args in args.heads.items():\n",
    "        logging.info(f\"=============    Processing head {head}     ===========\")\n",
    "        head_config = dict_head_to_dataclass(head_args, head, args)\n",
    "        # if statistics file is given, use\n",
    "        if head_config.statistics_file is not None:\n",
    "            with open(head_config.statistics_file, \"r\") as f:  # pylint: disable=W1514\n",
    "                statistics = json.load(f)\n",
    "            logging.info(\"Using statistics json file\")\n",
    "            head_config.r_max = (\n",
    "                statistics[\"r_max\"] if args.foundation_model is None else args.r_max\n",
    "            )\n",
    "            head_config.atomic_numbers = statistics[\"atomic_numbers\"]\n",
    "            head_config.mean = statistics[\"mean\"]\n",
    "            head_config.std = statistics[\"std\"]\n",
    "            head_config.avg_num_neighbors = statistics[\"avg_num_neighbors\"]\n",
    "            head_config.compute_avg_num_neighbors = False\n",
    "            if isinstance(statistics[\"atomic_energies\"], str) and statistics[\"atomic_energies\"].endswith(\".json\"):\n",
    "                with open(statistics[\"atomic_energies\"], \"r\", encoding=\"utf-8\") as f:\n",
    "                    atomic_energies = json.load(f)\n",
    "                head_config.E0s = atomic_energies\n",
    "                head_config.atomic_energies_dict = ast.literal_eval(atomic_energies)\n",
    "            else:\n",
    "                head_config.E0s = statistics[\"atomic_energies\"]\n",
    "                head_config.atomic_energies_dict = ast.literal_eval(\n",
    "                    statistics[\"atomic_energies\"]\n",
    "                )\n",
    "\n",
    "        # Data preparation, if train_file is .d5 or .hdf5 or dir that contains it, or empty dir => False\n",
    "        if check_path_ase_read(head_config.train_file):\n",
    "            if head_config.valid_file is not None:\n",
    "                assert check_path_ase_read(head_config.valid_file), \"valid_file if given must be same format as train_file\"\n",
    "\n",
    "            config_type_weights = get_config_type_weights(head_config.config_type_weights)\n",
    "            collections, atomic_energies_dict = get_dataset_from_xyz(\n",
    "                work_dir=args.work_dir,\n",
    "                train_path=head_config.train_file,\n",
    "                valid_path=head_config.valid_file,\n",
    "                valid_fraction=head_config.valid_fraction,\n",
    "                config_type_weights=config_type_weights,\n",
    "                test_path=head_config.test_file,\n",
    "                seed=args.seed,\n",
    "                energy_key=head_config.energy_key,\n",
    "                forces_key=head_config.forces_key,\n",
    "                stress_key=head_config.stress_key,\n",
    "                virials_key=head_config.virials_key,\n",
    "                dipole_key=head_config.dipole_key,\n",
    "                charges_key=head_config.charges_key,\n",
    "                head_name=head_config.head_name,\n",
    "                keep_isolated_atoms=head_config.keep_isolated_atoms,\n",
    "            )\n",
    "            head_config.collections = collections\n",
    "            head_config.atomic_energies_dict = atomic_energies_dict\n",
    "            logging.info(f\"Total number of configurations: train={len(collections.train)}, valid={len(collections.valid)}, \"\n",
    "                        f\"tests=[{', '.join([name + ': ' + str(len(test_configs)) for name, test_configs in collections.tests])}],\")\n",
    "        head_configs.append(head_config)\n",
    "\n",
    "    # check if enough number of samples are there\n",
    "    if all(check_path_ase_read(head_config.train_file) for head_config in head_configs):\n",
    "        size_collections_train = sum(len(head_config.collections.train) for head_config in head_configs)\n",
    "        size_collections_valid = sum(len(head_config.collections.valid) for head_config in head_configs)\n",
    "        if size_collections_train < args.batch_size:\n",
    "            logging.error(f\"Batch size ({args.batch_size}) is larger than the number of training data ({size_collections_train})\")\n",
    "        if size_collections_valid < args.valid_batch_size:\n",
    "            logging.warning(f\"Validation batch size ({args.valid_batch_size}) is larger than the number of validation data ({size_collections_valid})\")\n",
    "\n",
    "    # if we are usng multihead finetuning, load pretrain data as well\n",
    "    if args.multiheads_finetuning:\n",
    "        logging.info(\"==================Using multiheads finetuning mode==================\")\n",
    "        args.loss = \"universal\"\n",
    "        \n",
    "        logging.info(f\"Using foundation model for multiheads finetuning with {args.pt_train_file}\")\n",
    "        # add pretraining head at start\n",
    "        heads = list(dict.fromkeys([\"pt_head\"] + heads))\n",
    "        # load data\n",
    "        collections, atomic_energies_dict = get_dataset_from_xyz(\n",
    "            work_dir=args.work_dir,\n",
    "            train_path=args.pt_train_file,\n",
    "            valid_path=args.pt_valid_file,\n",
    "            valid_fraction=args.valid_fraction,\n",
    "            config_type_weights=None,\n",
    "            test_path=None,\n",
    "            seed=args.seed,\n",
    "            energy_key=args.energy_key,\n",
    "            forces_key=args.forces_key,\n",
    "            stress_key=args.stress_key,\n",
    "            virials_key=args.virials_key,\n",
    "            dipole_key=args.dipole_key,\n",
    "            charges_key=args.charges_key,\n",
    "            head_name=\"pt_head\",\n",
    "            keep_isolated_atoms=args.keep_isolated_atoms,\n",
    "        )\n",
    "\n",
    "        # create pretrain head\n",
    "        head_config_pt = HeadConfig(\n",
    "            head_name=\"pt_head\",\n",
    "            train_file=args.pt_train_file,\n",
    "            valid_file=args.pt_valid_file,\n",
    "            E0s=\"foundation\",\n",
    "            statistics_file=args.statistics_file,\n",
    "            valid_fraction=args.valid_fraction,\n",
    "            config_type_weights=None,\n",
    "            energy_key=args.energy_key,\n",
    "            forces_key=args.forces_key,\n",
    "            stress_key=args.stress_key,\n",
    "            virials_key=args.virials_key,\n",
    "            dipole_key=args.dipole_key,\n",
    "            charges_key=args.charges_key,\n",
    "            keep_isolated_atoms=args.keep_isolated_atoms,\n",
    "            collections=collections,\n",
    "            avg_num_neighbors=model_foundation.interactions[0].avg_num_neighbors,\n",
    "            compute_avg_num_neighbors=False,\n",
    "        )\n",
    "        head_config_pt.collections = collections\n",
    "        head_configs.append(head_config_pt)\n",
    "        logging.info(f\"Total number of configurations: train={len(collections.train)}, valid={len(collections.valid)}\")\n",
    "\n",
    "    # Atomic number table\n",
    "    # yapf: disable\n",
    "    for head_config in head_configs:\n",
    "        # if atomic numbers not given, extract from train and valid datasets\n",
    "        if head_config.atomic_numbers is None:            \n",
    "            z_table_head = tools.get_atomic_number_table_from_zs(\n",
    "                z\n",
    "                for configs in (head_config.collections.train, head_config.collections.valid)\n",
    "                for config in configs\n",
    "                for z in config.atomic_numbers\n",
    "            )\n",
    "            head_config.atomic_numbers = z_table_head.zs\n",
    "            head_config.z_table = z_table_head\n",
    "        else:\n",
    "            # if given, but not in stat file, read from command line\n",
    "            if head_config.statistics_file is None:\n",
    "                logging.info(\"Using atomic numbers from command line argument\")\n",
    "            else:\n",
    "                logging.info(\"Using atomic numbers from statistics file\")\n",
    "            zs_list = ast.literal_eval(head_config.atomic_numbers)\n",
    "            assert isinstance(zs_list, list)\n",
    "            z_table_head = tools.AtomicNumberTable(zs_list)\n",
    "            head_config.atomic_numbers = zs_list\n",
    "            head_config.z_table = z_table_head\n",
    "        # yapf: enable\n",
    "\n",
    "    #  pool all atomic numbers from all heads\n",
    "    all_atomic_numbers = set()\n",
    "    for head_config in head_configs:\n",
    "        all_atomic_numbers.update(head_config.atomic_numbers)\n",
    "    z_table = AtomicNumberTable(sorted(list(all_atomic_numbers)))\n",
    "    logging.info(f\"Atomic Numbers used: {z_table.zs}\")\n",
    "\n",
    "    # Atomic energies\n",
    "    atomic_energies_dict = {}\n",
    "    for head_config in head_configs:\n",
    "        # if no atomic energy dict is given\n",
    "        if head_config.atomic_energies_dict is None or len(head_config.atomic_energies_dict) == 0:\n",
    "            # E0 can't be none then\n",
    "            assert head_config.E0s is not None, \"Atomic energies must be provided\"\n",
    "            # if not foundation, calculate if train file given\n",
    "            if check_path_ase_read(head_config.train_file) and head_config.E0s.lower() != \"foundation\":\n",
    "                atomic_energies_dict[head_config.head_name] = get_atomic_energies(head_config.E0s,\n",
    "                                                                                head_config.collections.train, \n",
    "                                                                                head_config.z_table)\n",
    "            # if E0 to be used from foundation\n",
    "            elif head_config.E0s.lower() == \"foundation\":\n",
    "                assert args.foundation_model is not None\n",
    "                z_table_foundation = AtomicNumberTable([int(z) for z in model_foundation.atomic_numbers])\n",
    "                foundation_atomic_energies = model_foundation.atomic_energies_fn.atomic_energies\n",
    "\n",
    "                # if foundation model is multihead\n",
    "                if foundation_atomic_energies.ndim > 1:\n",
    "                    foundation_atomic_energies = foundation_atomic_energies.squeeze()\n",
    "                    if foundation_atomic_energies.ndim == 2:\n",
    "                        foundation_atomic_energies = foundation_atomic_energies[0]\n",
    "                        logging.info(\"Foundation model has multiple heads, using the first head as foundation E0s.\")\n",
    "                atomic_energies_dict[head_config.head_name] = {z: foundation_atomic_energies[z_table_foundation.z_to_index(z)].item() for z in z_table.zs}\n",
    "            else:\n",
    "                # if train file not given, may have to read from json\n",
    "                atomic_energies_dict[head_config.head_name] = get_atomic_energies(head_config.E0s, None, head_config.z_table)\n",
    "        # use the given atomic energy dict\n",
    "        else:\n",
    "            atomic_energies_dict[head_config.head_name] = head_config.atomic_energies_dict\n",
    "\n",
    "    # Atomic energies for multiheads finetuning, for pretrain head\n",
    "    if args.multiheads_finetuning:\n",
    "        assert model_foundation is not None, \"Model foundation must be provided for multiheads finetuning\"\n",
    "        z_table_foundation = AtomicNumberTable([int(z) for z in model_foundation.atomic_numbers])\n",
    "        foundation_atomic_energies = model_foundation.atomic_energies_fn.atomic_energies\n",
    "\n",
    "        # if the foundation model is itself multihead take first head/which is usully it's pretrained head, as we add pt_head at start\n",
    "        if foundation_atomic_energies.ndim > 1:\n",
    "            foundation_atomic_energies = foundation_atomic_energies.squeeze()\n",
    "            if foundation_atomic_energies.ndim == 2:\n",
    "                foundation_atomic_energies = foundation_atomic_energies[0]\n",
    "                logging.info(\"Foundation model has multiple heads, using the first head as foundation E0s.\")\n",
    "        atomic_energies_dict[\"pt_head\"] = { z: foundation_atomic_energies[z_table_foundation.z_to_index(z)].item()for z in z_table.zs}\n",
    "\n",
    "    # set the output args\n",
    "    dipole_only = False\n",
    "    args.compute_energy = True\n",
    "    args.compute_dipole = False\n",
    "    atomic_energies = dict_to_array(atomic_energies_dict, heads)\n",
    "\n",
    "    # log the atomic energies\n",
    "    for head_config in head_configs:\n",
    "        try:\n",
    "            logging.info(f\"Atomic Energies used (z: eV) for head {head_config.head_name}: \" + \"{\" + \", \".join([f\"{z}: {atomic_energies_dict[head_config.head_name][z]}\" for z in head_config.z_table.zs]) + \"}\")\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Atomic number {e} not found in atomic_energies_dict for head {head_config.head_name}, add E0s for this atomic number\") from e\n",
    "\n",
    "\n",
    "    # get val and train sets\n",
    "    valid_sets = {head: [] for head in heads}\n",
    "    train_sets = {head: [] for head in heads}\n",
    "\n",
    "    for head_config in head_configs:\n",
    "        if check_path_ase_read(head_config.train_file):\n",
    "            train_sets[head_config.head_name] = [data.AtomicData.from_config( config, z_table=z_table, cutoff=args.r_max, heads=heads)\n",
    "                                                    for config in head_config.collections.train]\n",
    "            valid_sets[head_config.head_name] = [data.AtomicData.from_config( config, z_table=z_table, cutoff=args.r_max, heads=heads)\n",
    "                                                    for config in head_config.collections.valid]\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Provide file that ends with .xyz instead of {head_config.train_file}\")\n",
    "        \n",
    "        train_loader_head = torch_geometric.dataloader.DataLoader(\n",
    "            dataset=train_sets[head_config.head_name],\n",
    "            batch_size=args.batch_size,\n",
    "            shuffle=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=args.pin_memory,\n",
    "            num_workers=args.num_workers,\n",
    "            generator=torch.Generator().manual_seed(args.seed),\n",
    "        )\n",
    "        head_config.train_loader = train_loader_head\n",
    "        \n",
    "    # concatenate all the trainsets\n",
    "    train_set = ConcatDataset([train_sets[head] for head in heads])\n",
    "    \n",
    "    train_loader = torch_geometric.dataloader.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=args.pin_memory,\n",
    "        num_workers=args.num_workers,\n",
    "        generator=torch.Generator().manual_seed(args.seed),\n",
    "    )\n",
    "    # valid loaders will be different for each head\n",
    "    valid_loaders = {heads[i]: None for i in range(len(heads))}\n",
    "    if not isinstance(valid_sets, dict):\n",
    "        valid_sets = {\"Default\": valid_sets}\n",
    "\n",
    "    for head, valid_set in valid_sets.items():\n",
    "        valid_loaders[head] = torch_geometric.dataloader.DataLoader(\n",
    "            dataset=valid_set,\n",
    "            batch_size=args.valid_batch_size,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            pin_memory=args.pin_memory,\n",
    "            num_workers=args.num_workers,\n",
    "            generator=torch.Generator().manual_seed(args.seed),\n",
    "        )\n",
    "\n",
    "    # get loss function and avg num of neighbors\n",
    "    loss_fn = get_loss_fn(args, dipole_only, args.compute_dipole)\n",
    "    args.avg_num_neighbors = get_avg_num_neighbors(head_configs, args, train_loader, device)\n",
    "\n",
    "    # Model\n",
    "    model, output_args = configure_model(args, train_loader, atomic_energies, model_foundation, heads, z_table)\n",
    "    model.to(device)\n",
    "\n",
    "    logging.debug(model)\n",
    "    logging.info(f\"Total number of parameters: {tools.count_parameters(model)}\")\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"===========OPTIMIZER INFORMATION===========\")\n",
    "    logging.info(f\"Using {args.optimizer.upper()} as parameter optimizer\")\n",
    "    logging.info(f\"Batch size: {args.batch_size}\")\n",
    "    if args.ema:\n",
    "        logging.info(f\"Using Exponential Moving Average with decay: {args.ema_decay}\")\n",
    "    logging.info(f\"Number of gradient updates: {int(args.max_num_epochs*len(train_set)/args.batch_size)}\")\n",
    "    logging.info(f\"Learning rate: {args.lr}, weight decay: {args.weight_decay}\")\n",
    "    logging.info(loss_fn)\n",
    "\n",
    "    # Optimizer\n",
    "    param_options = get_params_options(args, model)\n",
    "    optimizer: torch.optim.Optimizer\n",
    "    optimizer = get_optimizer(args, param_options)\n",
    "    logger = tools.MetricsLogger(directory=args.results_dir, tag=tag + \"_train\")  # pylint: disable=E1123\n",
    "\n",
    "    # scheduler and swa\n",
    "    lr_scheduler = LRScheduler(optimizer, args)\n",
    "\n",
    "    swa: Optional[tools.SWAContainer] = None\n",
    "    swas = [False]\n",
    "    if args.swa:\n",
    "        swa, swas = get_swa(args, model, optimizer, swas, dipole_only)\n",
    "\n",
    "    # checkpoint handler\n",
    "    checkpoint_handler = tools.CheckpointHandler(\n",
    "        directory=args.checkpoints_dir,\n",
    "        tag=tag,\n",
    "        keep=args.keep_checkpoints,\n",
    "        swa_start=args.start_swa,\n",
    "    )\n",
    "\n",
    "    start_epoch = 0\n",
    "    if args.restart_latest:\n",
    "        try:\n",
    "            opt_start_epoch = checkpoint_handler.load_latest(state=tools.CheckpointState(model, optimizer, lr_scheduler),\n",
    "                                                             swa=True,\n",
    "                                                             device=device,)\n",
    "        except Exception:  # pylint: disable=W0703\n",
    "            opt_start_epoch = checkpoint_handler.load_latest( state=tools.CheckpointState(model, optimizer, lr_scheduler),\n",
    "                                                            swa=False,\n",
    "                                                            device=device,)\n",
    "        if opt_start_epoch is not None:\n",
    "            start_epoch = opt_start_epoch\n",
    "\n",
    "    # initialize ema\n",
    "    ema: Optional[ExponentialMovingAverage] = None\n",
    "    if args.ema:\n",
    "        ema = ExponentialMovingAverage(model.parameters(), decay=args.ema_decay)\n",
    "    else:\n",
    "        for group in optimizer.param_groups:\n",
    "            group[\"lr\"] = args.lr\n",
    "\n",
    "    train(\n",
    "        model=model,\n",
    "        loss_fn=loss_fn,\n",
    "        train_loader=train_loader,\n",
    "        valid_loaders=valid_loaders,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "        checkpoint_handler=checkpoint_handler,\n",
    "        eval_interval=args.eval_interval,\n",
    "        start_epoch=start_epoch,\n",
    "        max_num_epochs=args.max_num_epochs,\n",
    "        logger=logger,\n",
    "        patience=args.patience,\n",
    "        output_args=output_args,\n",
    "        device=device,\n",
    "        swa=swa,\n",
    "        ema=ema,\n",
    "        max_grad_norm=args.clip_grad,\n",
    "        log_errors=args.error_table,\n",
    "    )\n",
    "\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"===========RESULTS===========\")\n",
    "    logging.info(\"Computing metrics for training, validation, and test sets\")\n",
    "\n",
    "    # get train and all valid loaders for all heads\n",
    "    train_valid_data_loader = {}\n",
    "    for head_config in head_configs:\n",
    "        data_loader_name = \"train_\" + head_config.head_name\n",
    "        train_valid_data_loader[data_loader_name] = head_config.train_loader\n",
    "    for head, valid_loader in valid_loaders.items():\n",
    "        data_load_name = \"valid_\" + head\n",
    "        train_valid_data_loader[data_load_name] = valid_loader\n",
    "\n",
    "    test_sets = {}\n",
    "    stop_first_test = False # Initialize a flag to determine if testing should stop after the first test\n",
    "    test_data_loader = {}\n",
    "    # Check if all heads have the same test file and if it is not None\n",
    "    if all(head_config.test_file == head_configs[0].test_file for head_config in head_configs) and head_configs[0].test_file is not None:\n",
    "        stop_first_test = True\n",
    "    # Check if all heads have the same test directory and if it is not None\n",
    "    if all(head_config.test_dir == head_configs[0].test_dir for head_config in head_configs) and head_configs[0].test_dir is not None:\n",
    "        stop_first_test = True\n",
    "\n",
    "    # load test data head by head, stop if all heads have same test file\n",
    "    for head_config in head_configs:\n",
    "        if check_path_ase_read(head_config.train_file):\n",
    "            for name, subset in head_config.collections.tests:\n",
    "                test_sets[name] = [data.AtomicData.from_config(config, z_table=z_table, cutoff=args.r_max, heads=heads)\n",
    "                                    for config in subset]\n",
    "    \n",
    "        for test_name, test_set in test_sets.items():\n",
    "            test_loader = torch_geometric.dataloader.DataLoader(\n",
    "                test_set,\n",
    "                batch_size=args.valid_batch_size,\n",
    "                shuffle=False,\n",
    "                drop_last=False,\n",
    "                num_workers=args.num_workers,\n",
    "                pin_memory=args.pin_memory,\n",
    "            )\n",
    "            test_data_loader[test_name] = test_loader\n",
    "        if stop_first_test:\n",
    "            break\n",
    "\n",
    "    for swa_eval in swas:\n",
    "        epoch = checkpoint_handler.load_latest(state=tools.CheckpointState(model, optimizer, lr_scheduler),swa=swa_eval,device=device,)\n",
    "        model.to(device)\n",
    "\n",
    "        if swa_eval:\n",
    "            logging.info(f\"Loaded Stage two model from epoch {epoch} for evaluation\")\n",
    "        else:\n",
    "            logging.info(f\"Loaded Stage one model from epoch {epoch} for evaluation\")\n",
    "\n",
    "        # freeze model params\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        table_train_valid = create_error_table(\n",
    "                                                table_type=args.error_table,\n",
    "                                                all_data_loaders=train_valid_data_loader,\n",
    "                                                model=model,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                output_args=output_args,\n",
    "                                                log_wandb=False,\n",
    "                                                device=device,\n",
    "                                            )\n",
    "        logging.info(\"Error-table on TRAIN and VALID:\\n\" + str(table_train_valid))\n",
    "\n",
    "        if test_data_loader:\n",
    "            table_test = create_error_table(\n",
    "                                                table_type=args.error_table,\n",
    "                                                all_data_loaders=test_data_loader,\n",
    "                                                model=model,\n",
    "                                                loss_fn=loss_fn,\n",
    "                                                output_args=output_args,\n",
    "                                                log_wandb=False,\n",
    "                                                device=device,\n",
    "                                            )\n",
    "            logging.info(\"Error-table on TEST:\\n\" + str(table_test))\n",
    "\n",
    "        # Save entire model\n",
    "        if swa_eval:\n",
    "            model_path = Path(args.checkpoints_dir) / (tag + \"_stagetwo.model\")\n",
    "            torch.save(model, Path(args.model_dir) / (args.name + \"_stagetwo.model\"))\n",
    "            logging.info(f\"Saved stagetwo model at {Path(args.model_dir) / (args.name + \"_stagetwo.model\")}\")\n",
    "        else:\n",
    "            model_path = Path(args.checkpoints_dir) / (tag + \".model\")\n",
    "            torch.save(model, Path(args.model_dir) / (args.name + \".model\"))\n",
    "            logging.info(f\"Saved model at {Path(args.model_dir) / (args.name + \".model\")}\")\n",
    "\n",
    "        if args.save_cpu:\n",
    "            model = model.to(\"cpu\")\n",
    "        torch.save(model, model_path)\n",
    "\n",
    "        logging.info(f\"Saved model to {model_path}\")\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    logging.info(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import logging\n",
    "\n",
    "# import numpy as np\n",
    "# from e3nn import o3\n",
    "\n",
    "from mace import modules\n",
    "from mace.tools.finetuning_utils import load_foundations_elements\n",
    "from mace.tools.scripts_utils import extract_config_mace_model\n",
    "\n",
    "\n",
    "def configure_model(args, train_loader, atomic_energies, model_foundation=None, heads=None, z_table=None):\n",
    "    # Selecting outputs\n",
    "    compute_virials = args.loss in (\"stress\", \"virials\", \"huber\", \"universal\")\n",
    "    if compute_virials:\n",
    "        args.compute_stress = True\n",
    "        args.error_table = \"PerAtomRMSEstressvirials\"\n",
    "\n",
    "    output_args = {\n",
    "        \"energy\": args.compute_energy,\n",
    "        \"forces\": args.compute_forces,\n",
    "        \"virials\": compute_virials,\n",
    "        \"stress\": args.compute_stress,\n",
    "        \"dipoles\": args.compute_dipole,\n",
    "    }\n",
    "    logging.info(\n",
    "        f\"During training the following quantities will be reported: {', '.join([f'{report}' for report, value in output_args.items() if value])}\"\n",
    "    )\n",
    "    logging.info(\"===========MODEL DETAILS===========\")\n",
    "\n",
    "    if args.scaling == \"no_scaling\":\n",
    "        args.std = 1.0\n",
    "        logging.info(\"No scaling selected\")\n",
    "    elif (args.mean is None or args.std is None) and args.model != \"AtomicDipolesMACE\":\n",
    "        args.mean, args.std = modules.scaling_classes[args.scaling](\n",
    "            train_loader, atomic_energies\n",
    "        )\n",
    "\n",
    "    # Build model\n",
    "    if model_foundation is not None and args.model in [\"MACE\", \"ScaleShiftMACE\"]:\n",
    "        logging.info(\"Loading FOUNDATION model\")\n",
    "        model_config_foundation = extract_config_mace_model(model_foundation)\n",
    "        model_config_foundation[\"atomic_energies\"] = atomic_energies\n",
    "        model_config_foundation[\"atomic_numbers\"] = z_table.zs\n",
    "        model_config_foundation[\"num_elements\"] = len(z_table)\n",
    "        args.max_L = model_config_foundation[\"hidden_irreps\"].lmax\n",
    "\n",
    "        if args.model == \"MACE\" and model_foundation.__class__.__name__ == \"MACE\":\n",
    "            model_config_foundation[\"atomic_inter_shift\"] = [0.0] * len(heads)\n",
    "        else:\n",
    "            model_config_foundation[\"atomic_inter_shift\"] = (\n",
    "                _determine_atomic_inter_shift(args.mean, heads)\n",
    "            )\n",
    "        model_config_foundation[\"atomic_inter_scale\"] = [1.0] * len(heads)\n",
    "        args.avg_num_neighbors = model_config_foundation[\"avg_num_neighbors\"]\n",
    "        args.model = \"FoundationMACE\"\n",
    "        model_config_foundation[\"heads\"] = heads\n",
    "        model_config = model_config_foundation\n",
    "\n",
    "        logging.info(\"Model configuration extracted from foundation model\")\n",
    "        logging.info(\"Using universal loss function for fine-tuning\")\n",
    "        logging.info(\n",
    "            f\"Message passing with hidden irreps {model_config_foundation['hidden_irreps']})\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"{model_config_foundation['num_interactions']} layers, each with correlation order: {model_config_foundation['correlation']} (body order: {model_config_foundation['correlation']+1}) and spherical harmonics up to: l={model_config_foundation['max_ell']}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Radial cutoff: {model_config_foundation['r_max']} A (total receptive field for each atom: {model_config_foundation['r_max'] * model_config_foundation['num_interactions']} A)\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Distance transform for radial basis functions: {model_config_foundation['distance_transform']}\"\n",
    "        )\n",
    "    else:\n",
    "        logging.info(\"Building model\")\n",
    "        logging.info(\n",
    "            f\"Message passing with {args.num_channels} channels and max_L={args.max_L} ({args.hidden_irreps})\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"{args.num_interactions} layers, each with correlation order: {args.correlation} (body order: {args.correlation+1}) and spherical harmonics up to: l={args.max_ell}\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"{args.num_radial_basis} radial and {args.num_cutoff_basis} basis functions\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Radial cutoff: {args.r_max} A (total receptive field for each atom: {args.r_max * args.num_interactions} A)\"\n",
    "        )\n",
    "        logging.info(\n",
    "            f\"Distance transform for radial basis functions: {args.distance_transform}\"\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            len({irrep.mul for irrep in o3.Irreps(args.hidden_irreps)}) == 1\n",
    "        ), \"All channels must have the same dimension, use the num_channels and max_L keywords to specify the number of channels and the maximum L\"\n",
    "\n",
    "        logging.info(f\"Hidden irreps: {args.hidden_irreps}\")\n",
    "\n",
    "        model_config = dict(\n",
    "            r_max=args.r_max,\n",
    "            num_bessel=args.num_radial_basis,\n",
    "            num_polynomial_cutoff=args.num_cutoff_basis,\n",
    "            max_ell=args.max_ell,\n",
    "            interaction_cls=modules.interaction_classes[args.interaction],\n",
    "            num_interactions=args.num_interactions,\n",
    "            num_elements=len(z_table),\n",
    "            hidden_irreps=o3.Irreps(args.hidden_irreps),\n",
    "            atomic_energies=atomic_energies,\n",
    "            avg_num_neighbors=args.avg_num_neighbors,\n",
    "            atomic_numbers=z_table.zs,\n",
    "        )\n",
    "        model_config_foundation = None\n",
    "\n",
    "    model = _build_model(args, model_config, model_config_foundation, heads)\n",
    "\n",
    "    if model_foundation is not None:\n",
    "        model = load_foundations_elements(\n",
    "            model,\n",
    "            model_foundation,\n",
    "            z_table,\n",
    "            load_readout=args.foundation_filter_elements,\n",
    "            max_L=args.max_L,\n",
    "        )\n",
    "\n",
    "    return model, output_args\n",
    "\n",
    "\n",
    "def _determine_atomic_inter_shift(mean, heads):\n",
    "    if isinstance(mean, np.ndarray):\n",
    "        if mean.size == 1:\n",
    "            return mean.item()\n",
    "        if mean.size == len(heads):\n",
    "            return mean.tolist()\n",
    "        logging.info(\"Mean not in correct format, using default value of 0.0\")\n",
    "        return [0.0] * len(heads)\n",
    "    if isinstance(mean, list) and len(mean) == len(heads):\n",
    "        return mean\n",
    "    if isinstance(mean, float):\n",
    "        return [mean] * len(heads)\n",
    "    logging.info(\"Mean not in correct format, using default value of 0.0\")\n",
    "    return [0.0] * len(heads)\n",
    "\n",
    "\n",
    "def _build_model(args, model_config, model_config_foundation, heads):  # pylint: disable=too-many-return-statements\n",
    "    if args.model == \"MACE\":\n",
    "        return ScaleShiftMACE(\n",
    "            **model_config,\n",
    "            pair_repulsion=args.pair_repulsion,\n",
    "            distance_transform=args.distance_transform,\n",
    "            correlation=args.correlation,\n",
    "            gate=modules.gate_dict[args.gate],\n",
    "            interaction_cls_first=modules.interaction_classes[\n",
    "                \"RealAgnosticInteractionBlock\"\n",
    "            ],\n",
    "            MLP_irreps=o3.Irreps(args.MLP_irreps),\n",
    "            atomic_inter_scale=args.std,\n",
    "            atomic_inter_shift=[0.0] * len(heads),\n",
    "            radial_MLP=ast.literal_eval(args.radial_MLP),\n",
    "            radial_type=args.radial_type,\n",
    "            heads=heads,\n",
    "        )\n",
    "    if args.model == \"ScaleShiftMACE\":\n",
    "        return ScaleShiftMACE(\n",
    "            **model_config,\n",
    "            pair_repulsion=args.pair_repulsion,\n",
    "            distance_transform=args.distance_transform,\n",
    "            correlation=args.correlation,\n",
    "            gate=modules.gate_dict[args.gate],\n",
    "            interaction_cls_first=modules.interaction_classes[args.interaction_first],\n",
    "            MLP_irreps=o3.Irreps(args.MLP_irreps),\n",
    "            atomic_inter_scale=args.std,\n",
    "            atomic_inter_shift=args.mean,\n",
    "            radial_MLP=ast.literal_eval(args.radial_MLP),\n",
    "            radial_type=args.radial_type,\n",
    "            heads=heads,\n",
    "        )\n",
    "    if args.model == \"FoundationMACE\":\n",
    "        return ScaleShiftMACE(**model_config_foundation)\n",
    "    \n",
    "    raise RuntimeError(f\"Unknown model: '{args.model}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mace_0.3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
